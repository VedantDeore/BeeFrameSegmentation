{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute mAP for object detection\n",
    "def compute_map(pred_boxes, pred_labels, pred_scores, true_boxes, true_labels, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes mean Average Precision (mAP) for object detection.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_boxes: List of predicted bounding boxes for all images\n",
    "    - pred_labels: List of predicted labels for all images\n",
    "    - pred_scores: List of predicted scores for all images\n",
    "    - true_boxes: List of ground truth bounding boxes for all images\n",
    "    - true_labels: List of ground truth labels for all images\n",
    "    - iou_threshold: IOU threshold for a positive match\n",
    "    \n",
    "    Returns:\n",
    "    - mAP value\n",
    "    \"\"\"\n",
    "    aps = []\n",
    "    for i in range(len(true_boxes)):\n",
    "        # Convert to tensors\n",
    "        pred_b = torch.tensor(pred_boxes[i], dtype=torch.float32)\n",
    "        true_b = torch.tensor(true_boxes[i], dtype=torch.float32)\n",
    "        pred_s = torch.tensor(pred_scores[i], dtype=torch.float32)\n",
    "\n",
    "        if len(pred_b) == 0 or len(true_b) == 0:\n",
    "            aps.append(0)\n",
    "            continue\n",
    "\n",
    "        # Compute IoU\n",
    "        ious = box_iou(pred_b, true_b)\n",
    "\n",
    "        # Sort predictions by score\n",
    "        sorted_indices = torch.argsort(pred_s, descending=True)\n",
    "        ious = ious[sorted_indices]\n",
    "\n",
    "        # Calculate True Positives and False Positives\n",
    "        tp = (ious >= iou_threshold).sum(dim=1).clamp(max=1)\n",
    "        fp = 1 - tp\n",
    "\n",
    "        # Calculate Precision and Recall\n",
    "        tp_cumsum = torch.cumsum(tp, dim=0)\n",
    "        fp_cumsum = torch.cumsum(fp, dim=0)\n",
    "        recall = tp_cumsum / (len(true_b) + 1e-5)\n",
    "        precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-5)\n",
    "\n",
    "        # Calculate AP (Average Precision)\n",
    "        ap = torch.trapz(precision, recall)\n",
    "        aps.append(ap.item())\n",
    "\n",
    "    # Mean Average Precision\n",
    "    return sum(aps) / len(aps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute F1-Score for segmentation\n",
    "def compute_f1(true_masks, pred_masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes F1-Score for segmentation.\n",
    "    \n",
    "    Parameters:\n",
    "    - true_masks: Ground truth masks for all images\n",
    "    - pred_masks: Predicted masks for all images\n",
    "    - threshold: Threshold for binary classification\n",
    "    \n",
    "    Returns:\n",
    "    - F1-Score value\n",
    "    \"\"\"\n",
    "    true_flat = torch.flatten(torch.tensor(true_masks, dtype=torch.int))\n",
    "    pred_flat = torch.flatten((torch.tensor(pred_masks) > threshold).int())\n",
    "    f1 = f1_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data for demonstration\n",
    "pred_boxes = [\n",
    "    [[10, 10, 50, 50], [60, 60, 100, 100]],  # Image 1: 2 predicted boxes\n",
    "    [[25, 25, 75, 75]]                       # Image 2: 1 predicted box\n",
    "]\n",
    "\n",
    "true_boxes = [\n",
    "    [[12, 12, 52, 52], [58, 58, 98, 98]],  # Image 1: 2 ground truth boxes\n",
    "    [[26, 26, 76, 76]]                     # Image 2: 1 ground truth box\n",
    "]\n",
    "\n",
    "pred_labels = [[1, 1], [1]]   # Predicted labels for each box (dummy values, as they match true labels)\n",
    "true_labels = [[1, 1], [1]]   # Ground truth labels for each box (dummy values, as this is a simple case)\n",
    "pred_scores = [[0.9, 0.8], [0.85]]  # Predicted confidence scores\n",
    "\n",
    "pred_masks = [\n",
    "    torch.rand(1, 128, 128),  # Predicted mask for image 1\n",
    "    torch.rand(1, 128, 128)   # Predicted mask for image 2\n",
    "]\n",
    "\n",
    "true_masks = [\n",
    "    (torch.rand(1, 128, 128) > 0.5).int(),  # Ground truth mask for image 1 (binary mask)\n",
    "    (torch.rand(1, 128, 128) > 0.5).int()   # Ground truth mask for image 2 (binary mask)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mAP\n",
    "map_value = compute_map(pred_boxes, pred_labels, pred_scores, true_boxes, true_labels)\n",
    "print(f\"Mean Average Precision (mAP): {map_value:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1_value = compute_f1(true_masks, pred_masks)\n",
    "print(f\"F1-Score: {f1_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
